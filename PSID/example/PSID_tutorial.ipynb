{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isXmUBaxJanB"
   },
   "source": [
    "Written by: Omid G. Sani  \n",
    "Last update: June 17, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pXbrXQPKQRF"
   },
   "source": [
    "# What is PSID?\n",
    "\n",
    "PSID stands for preferential subspace identification, a method for dynamic modeling of time-series data, while prioritizing the dynamics shared with another time-series. \n",
    "\n",
    "For example, given signals $y_k$ (e.g. neural signals) and $z_k$ (e.g behavior), PSID learns a dynamic model for $y_k$ while prioritizing the dynamics that are relevant to $z_k$.\n",
    "\n",
    "For the derivation and results in real neural data see the paper below.\n",
    "\n",
    "**Publication:**\n",
    "\n",
    "Omid G. Sani, Hamidreza Abbaspourazad, Yan T. Wong, Bijan Pesaran, Maryam M. Shanechi. *Modeling behaviorally relevant neural dynamics enabled by preferential subspace identification*. Nature Neuroscience 24, 140â€“149 (2021). https://doi.org/10.1038/s41593-020-00733-0\n",
    "\n",
    "View-only full-text link: https://rdcu.be/b993t\n",
    "\n",
    "Original preprint: https://doi.org/10.1101/808154\n",
    "\n",
    "You can also find a summary of the paper in the following Twitter thread: https://twitter.com/MaryamShanechi/status/1325835609345122304"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSlPMSM5Ai3P"
   },
   "source": [
    "# Installing PSID\n",
    "To use PSID, you can either get the source code from [the PSID Github repository](https://github.com/ShanechiLab/PSID), or install it in your Python environment using pip:\n",
    "\n",
    "\n",
    "```\n",
    "pip install PSID\n",
    "```\n",
    "\n",
    "You can find the usage license in [LICENSE.md](https://github.com/ShanechiLab/PyPSID/blob/main/LICENSE.md). For this notebook, we will also start by installing PSID from pip."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZFHrfUEAYmC",
    "outputId": "2c807175-d8ba-4c0f-f75e-cc191c653876"
   },
   "source": [
    "!pip install PSID --upgrade"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsDpingpAhpK"
   },
   "source": [
    "# Using PSID\n",
    "## Modeling data\n",
    "To use PSID, you first need to import the library by running:\n",
    "```\n",
    "import PSID\n",
    "```\n",
    "You can then use its main data modeling function as:\n",
    "```\n",
    "idSys = PSID.PSID(y, z, nx, n1, i);\n",
    "```\n",
    "With the following arguments:\n",
    "- `y` and `z`: Neural (e.g. LFP signal powers or spike counts) and behavioral data (e.g. joint angles, hand position, etc), respectively. Dimensions are: time x data dimension (this can be changed with an optional argument documented in the code).\n",
    "- `nx`: the total dimension of the latent state in the model.\n",
    "- `n1`: the number of latent state dimensions that are going to be dedicated to behaviorally relevant neural dynamics.\n",
    "- `i`: the subspace horizon used for modeling. There is more on the choice of `i` later in this notebook, but numbers such as 5 or 10 are typically suitable values for `i`.\n",
    "\n",
    "And the following output:\n",
    "- `idSys`: an object containing all the learned model parameters ($A$, $C_y$, $C_z$, etc) and some prediction, etc methods. There is more on the model structure later in this notebook.\n",
    "\n",
    "## Using the model for dimension reduction, state estimation, and decoding\n",
    "For a learned PSID model `idSys`, you can use the `predict` method to extract the latent state and predict behavior and neural activity given any new neural data as:\n",
    "```\n",
    "zPred, yPred, xPred = idSys.predict(yTest)\n",
    "```\n",
    "With the argument:\n",
    "- `yTest`: Neural activity `y` in the test data. Dimensions are: time x data dimension.\n",
    "\n",
    "And outputs (all dimensions are time x data dimension):\n",
    "- `zPred`: Prediction of behavior using past neural activity at each data point.\n",
    "- `yPred`: Prediction of neural activity using past neural activity at each data point.\n",
    "- `xPred`: The latent state extarcted at each data point.\n",
    "\n",
    "We will next go through a complete example of using PSID in data.\n",
    "\n",
    "# A complete example\n",
    "In this example, we will use PSID to model some data. First, we import PSID and a few other useful tools from PSID and other libraries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bcBhkYe_Bt22"
   },
   "source": [
    "import argparse, sys, os\n",
    "\n",
    "sys.path.insert(0, os.path.join(\"..\", \"..\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "import PSID\n",
    "from PSID.evaluation import evalPrediction\n",
    "from PSID.MatHelper import loadmat"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading an example model:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DSKNITZB4r-",
    "outputId": "a33507ed-9df3-4041-d227-06b0e6317125"
   },
   "source": [
    "# Load data\n",
    "sample_model_path = os.path.join(\n",
    "    os.path.dirname(PSID.__file__), \"example\", \"sample_model.mat\"\n",
    ")\n",
    "\n",
    "print(\"Loading example model from {}\".format(sample_model_path))\n",
    "data = loadmat(sample_model_path)\n",
    "# This is an example model (shown in Supplementary Fig. 1) with\n",
    "# (a) 2 behaviorally relevant latent states,\n",
    "# (b) 2 behaviorally irrelevant latent states, and\n",
    "# (c) 2 states that drive behavior but are not represented in neural activity"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKjZYzGV4Oya"
   },
   "source": [
    "The PSID model looks like this:\n",
    "\n",
    "$$ x_{k+1} = A x_k + w_k $$  \n",
    "\n",
    "$$ y_k = C_y x_k + v_k $$  \n",
    "\n",
    "$$ z_k = C_z x_k + \\epsilon_k $$  \n",
    "\n",
    "where $y_k \\in \\!R^{n_y}$ is the neural activity, $z_k \\in \\!R^{n_z}$ is the behavior, and $x_k \\in \\!R^{n_x}$ is the latent state the describes the dynamics in both. Note that in general $y_k$ and $z_k$ could also be any other two signals (e.g. brain activity from two regions or even non-neural signals), but here we will refer these signals as neural activity and behavior, respectively. Importantly, PSID learns the model in the following format\n",
    "\n",
    "$$\n",
    "x_k = \\begin{bmatrix}\n",
    "x_k^{(1)} \\\\\n",
    "x_k^{(2)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where the behaviorally relevant dimensions of latent state ($x_k^{(1)} \\in \\!R^{n_1}$), which are those that drive $z_k$, are separated from the other dimensions ($x_k^{(2)} \\in \\!R^{n_2}$ with $n_2=n_x-n_1$). There are many equivalent ways of writing a latent state model such as this one, but PSID learns the one that uses minimal number of dimensions to explain behavior as parsimoniously as possible (you can find the precise definition in the paper). Critically, PSID can learn this minimal model (with only $x_k^{(1)}$) without having to also learn the rest of the model (the $x_k^{(2)}$ part). This is the concept of prioritization and allows PSID to learn the model more accurately, while requiring fewer training samples.\n",
    "\n",
    "\n",
    "Before going further, let's generate some sample data from this model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-9bAIpLC4LaE"
   },
   "source": [
    "# Generating some sample data from this model\n",
    "np.random.seed(42)  # For exact reproducibility\n",
    "\n",
    "N = int(2e4)\n",
    "trueSys = PSID.LSSM(params=data[\"trueSys\"])\n",
    "y, x = trueSys.generateRealization(N)\n",
    "z = (trueSys.Cz @ x.T).T\n",
    "\n",
    "# Add some z dynamics that are not encoded in y (i.e. epsilon)\n",
    "epsSys = PSID.LSSM(params=data[\"epsSys\"])\n",
    "eps, _ = epsSys.generateRealization(N)\n",
    "z += eps\n",
    "\n",
    "allYData, allZData = y, z"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above state-space model used by PSID, it is important for the neural/behavior data to be zero-mean. \n",
    "Starting version v1.1.0, PSID by default internally removes the mean from the neural/behavior data and adds\n",
    "it back to predictions, so the user does not need to handle this preprocessing. If the data is already zero-mean,\n",
    "this mean-removal will simply subtract and add zeros to signals so everything will still work.\n",
    "To cover this general case with data that is not zero-mean, let's artificially add some non-zero mean to the sample data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Just for this simulation, let's artificially add some non-zero mean to the sample data to cover the general case with non-zero-mean data:\n",
    "YMean = 10 * np.random.randn(allYData.shape[-1])\n",
    "ZMean = 10 * np.random.randn(allZData.shape[-1])\n",
    "allYData += YMean\n",
    "allZData += ZMean\n",
    "# Also reflect this in the true model:\n",
    "from PSID.PrepModel import PrepModel\n",
    "\n",
    "trueSys.YPrepModel = PrepModel(mean=YMean, remove_mean=True)\n",
    "trueSys.ZPrepModel = PrepModel(mean=ZMean, remove_mean=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQWYLQuwGWGY"
   },
   "source": [
    "Let's separate the data into training and test segments."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eiefXAMyExUa"
   },
   "source": [
    "# Separate data into training and test data:\n",
    "trainInds = np.arange(np.round(0.5 * allYData.shape[0]), dtype=int)\n",
    "testInds = np.arange(1 + trainInds[-1], allYData.shape[0])\n",
    "yTrain = allYData[trainInds, :]\n",
    "yTest = allYData[testInds, :]\n",
    "zTrain = allZData[trainInds, :]\n",
    "zTest = allZData[testInds, :]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_HZ9Xd_G7ew"
   },
   "source": [
    "We will next use PSID in two ways:  \n",
    "1. Learn a model with a low-dimensional latent state that only focuses on learning the behaviorally relevant neural dynamics (i.e. uses stage 1 of PSID only).  \n",
    "2. Learn a model that also learns other neural dynamics (i.e. uses both stages of PSID)\n",
    "\n",
    "We will then plot the learned models' eigenvalues (the eigenvalues of the $A$ matrix) to show that PSID learns the correct dynamics in each case.\n",
    "\n",
    "First, let's learn a model with a 2 dimensional latent state that only learns the behaviorally relevant neural dyanmics. For this, we pass the arguments nx=2 and n1=2 to the PSID function:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KeAWDK3sGfmr",
    "outputId": "39967d6f-e864-4236-8c87-8af7dd489f1e"
   },
   "source": [
    "## (Example 1) PSID can be used to dissociate and extract only the\n",
    "# behaviorally relevant latent states (with nx = n1 = 2)\n",
    "idSys1 = PSID.PSID(yTrain, zTrain, nx=2, n1=2, i=10)\n",
    "# You can also use the time_first=False argument if time is the second dimension:\n",
    "# idSys1 = PSID.PSID(yTrain.T, zTrain.T, nx=2, n1=2, i=10, time_first=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PSID learning function returns an object (here idSys1) that contains the learned model parameters and can be used to extract the latent states and decode behavior in new data. To do this, we use the 'predict' method in the learned model:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predict behavior using the learned model\n",
    "zTestPred1, yTestPred1, xTestPred1 = idSys1.predict(yTest)\n",
    "\n",
    "# Compute R2 of decoding\n",
    "R2 = evalPrediction(zTest, zTestPred1, \"R2\")\n",
    "\n",
    "# Predict behavior using the true model for comparison\n",
    "zTestPredIdeal, yTestPredIdeal, xTestPredIdeal = trueSys.predict(yTest)\n",
    "R2Ideal = evalPrediction(zTest, zTestPredIdeal, \"R2\")\n",
    "\n",
    "print(\n",
    "    \"Behavior decoding R2:\\n  PSID => {:.3g}, Ideal using true model => {:.3g}\".format(\n",
    "        np.mean(R2), np.mean(R2Ideal)\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfhHQ3hQKkeq"
   },
   "source": [
    "We can see that the PSID model with a 2D latent state is as accurate in explaining behavior as the full model that has a 4D latent state. This is because the other 2 latent state dimensions in the true model explain dynamics that are exclusive to neural activity (i.e. are not behaviorally relevant).\n",
    "\n",
    "Optionally, PSID can also learn other latent states beyond the behaviorally relevant ones. For this, we pass the arguments nx=4 and n1=2 to the PSID function:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2chTfdSKW5d",
    "outputId": "2ad2af37-c80b-4a1b-dfba-9039a56ce8e0"
   },
   "source": [
    "## (Example 2) Optionally, PSID can additionally also learn the\n",
    "# behaviorally irrelevant latent states (with nx = 4, n1 = 2)\n",
    "idSys2 = PSID.PSID(yTrain, zTrain, nx=4, n1=2, i=10)\n",
    "\n",
    "# In addition to ideal behavior decoding, this model will also have ideal neural self-prediction\n",
    "zTestPred2, yTestPred2, xTestPred2 = idSys2.predict(yTest)\n",
    "yR22 = evalPrediction(yTest, yTestPred2, \"R2\")\n",
    "yR2Ideal = evalPrediction(yTest, yTestPredIdeal, \"R2\")\n",
    "print(\n",
    "    \"Neural self-prediction R2:\\n  PSID => {:.3g}, Ideal using true model => {:.3g}\".format(\n",
    "        np.mean(yR22), np.mean(yR2Ideal)\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDvqwXjRLKgj"
   },
   "source": [
    "We can see that in this case, the model learned by PSID (which now has a 4D latent state) is also as good as the true model in terms of explaining neural activity. \n",
    "\n",
    "\n",
    "Finally, we can plot the eigenvalues of the $A$ matrix in each of the learned models and compare them with the eigenvalues of the $A$ matrix in the true model to see the accurate learning of behaviorally relevant (and optionally the other) dynamics by PSID. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "rglPnehvLJ-2",
    "outputId": "57690508-4203-4361-bd15-25056eff6395"
   },
   "source": [
    "# #########################################\n",
    "# Plot the true and identified eigenvalues\n",
    "\n",
    "# (Example 1) Eigenvalues when only learning behaviorally relevant states\n",
    "idEigs1 = np.linalg.eig(idSys1.A)[0]\n",
    "\n",
    "# (Example 2) Additional eigenvalues when also learning behaviorally irrelevant states\n",
    "# The identified model is already in form of Eq. 4, with behaviorally irrelevant states\n",
    "# coming as the last 2 dimensions of the states in the identified model\n",
    "idEigs2 = np.linalg.eig(idSys2.A[2:, 2:])[0]\n",
    "\n",
    "relevantDims = (\n",
    "    trueSys.zDims - 1\n",
    ")  # Dimensions that drive both behavior and neural activity\n",
    "irrelevantDims = [\n",
    "    x for x in np.arange(trueSys.state_dim, dtype=int) if x not in relevantDims\n",
    "]  # Dimensions that only drive the neural activity\n",
    "trueEigsRelevant = np.linalg.eig(trueSys.A[np.ix_(relevantDims, relevantDims)])[0]\n",
    "trueEigsIrrelevant = np.linalg.eig(trueSys.A[np.ix_(irrelevantDims, irrelevantDims)])[0]\n",
    "nonEncodedEigs = np.linalg.eig(data[\"epsSys\"][\"a\"])[\n",
    "    0\n",
    "]  # Eigenvalues for states that only drive behavior\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "axs = fig.subplots(1, 2)\n",
    "axs[1].remove()\n",
    "ax = axs[0]\n",
    "ax.axis(\"equal\")\n",
    "ax.add_patch(\n",
    "    patches.Circle((0, 0), radius=1, fill=False, color=\"black\", alpha=0.2, ls=\"-\")\n",
    ")\n",
    "ax.plot([-1, 1, 0, 0, 0], [0, 0, 0, -1, 1], color=\"black\", alpha=0.2, ls=\"-\")\n",
    "ax.scatter(\n",
    "    np.real(nonEncodedEigs),\n",
    "    np.imag(nonEncodedEigs),\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"#0000ff\",\n",
    "    facecolors=\"none\",\n",
    "    label=\"Not encoded in neural signals\",\n",
    ")\n",
    "ax.scatter(\n",
    "    np.real(trueEigsIrrelevant),\n",
    "    np.imag(trueEigsIrrelevant),\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"#ff0000\",\n",
    "    facecolors=\"none\",\n",
    "    label=\"Behaviorally irrelevant\",\n",
    ")\n",
    "ax.scatter(\n",
    "    np.real(trueEigsRelevant),\n",
    "    np.imag(trueEigsRelevant),\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"#00ff00\",\n",
    "    facecolors=\"none\",\n",
    "    label=\"Behaviorally relevant\",\n",
    ")\n",
    "ax.scatter(\n",
    "    np.real(idEigs1),\n",
    "    np.imag(idEigs1),\n",
    "    marker=\"x\",\n",
    "    facecolors=\"#00aa00\",\n",
    "    label=\"PSID Identified (stage 1)\",\n",
    ")\n",
    "ax.scatter(\n",
    "    np.real(idEigs2),\n",
    "    np.imag(idEigs2),\n",
    "    marker=\"x\",\n",
    "    facecolors=\"#aa0000\",\n",
    "    label=\"(optional) PSID Identified (stage 2)\",\n",
    ")\n",
    "ax.set_title(\"True and identified eigevalues\")\n",
    "ax.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIg0PhNeFcyj"
   },
   "source": [
    "# Using PSID with trial based data\n",
    "You can also use PSID if the data is available in separate chunks, for example across many trials. To do this, simply pass a python list with the data in each chunk/trial as the argument to PSID. The trials don't need to have the same number of samples either. \n",
    "\n",
    "Below is an example, where we break the same data as before in small chunks of random length, and then pass it to PSID."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-McGIeBhE90J",
    "outputId": "91fa16cf-aa58-4688-e695-5c675dd12886"
   },
   "source": [
    "## (Example 3) PSID can be used if data is available in discontinuous segments (e.g. different trials)\n",
    "# In this case, y and z data segments must be provided as elements of a list\n",
    "# Trials do not need to have the same number of samples\n",
    "# Here, for example assume that trials start at every 1000 samples.\n",
    "# And each each trial has a random length of 500 to 900 samples\n",
    "trialStartInds = np.arange(0, allYData.shape[0] - 1000, 1000)\n",
    "trialDurRange = np.array([900, 990])\n",
    "trialDur = np.random.randint(\n",
    "    low=trialDurRange[0], high=1 + trialDurRange[1], size=trialStartInds.shape\n",
    ")\n",
    "trialInds = [\n",
    "    trialStartInds[ti] + np.arange(trialDur[ti]) for ti in range(trialStartInds.size)\n",
    "]\n",
    "yTrials = [allYData[trialIndsThis, :] for trialIndsThis in trialInds]\n",
    "zTrials = [allZData[trialIndsThis, :] for trialIndsThis in trialInds]\n",
    "\n",
    "# Separate data into training and test data:\n",
    "trainInds = np.arange(np.round(0.5 * len(yTrials)), dtype=int)\n",
    "testInds = np.arange(1 + trainInds[-1], len(yTrials))\n",
    "yTrain = [yTrials[ti] for ti in trainInds]\n",
    "yTest = [yTrials[ti] for ti in testInds]\n",
    "zTrain = [zTrials[ti] for ti in trainInds]\n",
    "zTest = [zTrials[ti] for ti in testInds]\n",
    "\n",
    "idSys3 = PSID.PSID(yTrain, zTrain, nx=2, n1=2, i=10)\n",
    "\n",
    "for ti in range(len(yTest)):\n",
    "    zPredThis, yPredThis, xPredThis = idSys3.predict(yTest[ti])\n",
    "    zPredThisIdeal, yPredThisIdeal, xPredThisIdeal = trueSys.predict(yTest[ti])\n",
    "    if ti == 0:\n",
    "        zTestA = zTest[ti]\n",
    "        zPredA = zPredThis\n",
    "        zPredIdealA = zPredThisIdeal\n",
    "    else:\n",
    "        zTestA = np.concatenate((zTestA, zTest[ti]), axis=0)\n",
    "        zPredA = np.concatenate((zPredA, zPredThis), axis=0)\n",
    "        zPredIdealA = np.concatenate((zPredIdealA, zPredThisIdeal), axis=0)\n",
    "\n",
    "R2TrialBased = evalPrediction(zTestA, zPredA, \"R2\")\n",
    "R2TrialBasedIdeal = evalPrediction(zTestA, zPredIdealA, \"R2\")\n",
    "\n",
    "print(\n",
    "    \"Behavior decoding R2 (trial-based learning/decoding):\\n  PSID => {:.3g}, Ideal using true model = {:.3g}\".format(\n",
    "        np.mean(R2TrialBased), np.mean(R2TrialBasedIdeal)\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to pick the state dimensions `nx` and `n1`?\n",
    "`nx` determines the total dimension of the latent state and `n1` determines how many of those dimensions will be prioritizing the inclusion of behaviorally relevant neural dynamics (i.e. will be extracted using stage 1 of PSID). So the values that you would select for these hyperparameters depend on the goal of modeling and on the data. Some examples use cases are:\n",
    "- If you want to perform dimension reduction, `nx` will be your desired target dimension. For example, to reduce dimension to 2 to plot low-dimensional visualizations of neural activity, you would use `nx=2`. Now if you want to reduce dimension while preserving as much behaviorally relevant neural dynamics as possible, you would use `n1=nx`. \n",
    "- If you want to find the best fit to data overall, you can perform a grid search over values of `nx` and `n1` and pick the value that achieves the best performance metric in the training data. For example, you could pick the `nx` and `n1` pair that achieves the best cross-validated behavior decoding in an inner-cross-validation within the training data.\n",
    "\n",
    "# How to pick the horizon `i`?\n",
    "The horizon `i` does not affect the model structure and only affects the intermediate linear algebra operations that PSID performs during the learning of the model. Nevertheless, different values of `i` may have different model learning performance. `i` needs to be at least 2, but also also determines the maximum `n1` and `nx` that can be used per: \n",
    "```\n",
    "n1 <= nz * i\n",
    "nx <= ny * i\n",
    "```\n",
    "So if you have a low dimensional y or z, you typically would choose larger values for `i`, and vice versa. It is also possible to select the best performing `i` via an inner cross-validation approach similar to `nx` and `n1` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Licence\n",
    "Copyright (c) 2020 University of Southern California  \n",
    "See full notice in [LICENSE.md](https://github.com/ShanechiLab/PyPSID/blob/main/LICENSE.md)  \n",
    "Omid G. Sani and Maryam M. Shanechi  \n",
    "Shanechi Lab, University of Southern California"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PSID_example.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.11.4 ('py311tf213')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "f370938e660bea469f131c613689d5c02e8c75f5127e5a1eea02754ce8f2c2f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
